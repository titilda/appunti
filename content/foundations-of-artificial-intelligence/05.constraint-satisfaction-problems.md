---
title: "Chapter 4: Constraint Satisfaction Problems"
description: "Constraint satisfaction problems, backtracking search, local search, and constraint propagation techniques"
authors:
  - "Andrea Lunghi"
  - "Niccol√≤ Papini"
slug: "constraint-satisfaction-problems"
---

A **Constraint Satisfaction Problem** (CSP) is a problem where the goal is to find a solution that satisfies a set of constraints.

CSP use general heuristics to solve problems that allows to prune a large portion of the search space that violates the constraints.

Each state is represented by a set of variables, each with a domain of possible values, and a set of constraints that limit the possible values of the variables.

- **Variable** ($X$), {$X_1$, ..., $X_n$}: A symbol that represents a value.
- **Domain** ($D$), {$D_1$, ..., $D_n$}: The set of possible values {$v_1$, ..., $v_k$} that a variable can take.
- **Constraint** ($C$), {$C_1$, ..., $C_n$}: A restriction on the possible values of the variables. Each constraint is a pair {_scope_, _rel_}, where _scope_ is a set of variables that are involved in the constraint and _rel_ is a relation that defines the values that the variable can take. There are four types of constraints:
  - **Unary Constraint**: A constraint that involves a single variable.
  - **Binary Constraint**: A constraint that involves two variables.
  - **Higher-Order Constraint**: A constraint that involves more than two variables. This can be represented as a set of binary constraints.
  - **Global Constraint**: A constraint that involves an arbitrary number of variables.

CSP assigns a value to each variable in a way that satisfies all the constraints.
An assignment that satisfies all the constraints is called a **Consistent** assignment.
An assignment that assigns a value to each variable is called a **Complete** assignment.
A **Solution** is a complete and consistent assignment.
It is possible to have a **Partial Assignment**, which assigns a value to some variables but not all of them, and a **Partial Solution**, which is a partial assignment that is consistent.

CSP can be represented as a **Constraint Graph**, where the nodes are the variables and the edges are the constraints.

Some problems can include **Preference Constraints**, which are constraints that define the preference of a solution. These constraints are not necessary to find a solution, but they can help to find a better solution.
This type of problem are called **constraint optimization problems**.

## 4.1 Constraint Propagation

**Constraint Propagation** is a technique that allows to reduce the domain of the variables based on the constraints, ensuring _local consistency_.

There are different types of consistency:

### 4.1.1 Node Consistency

A variable is **node consistent** if it satisfies all the unary constraints that involve it.

### 4.1.2 Arc Consistency

A variable is **arc consistent** if it satisfies all the binary constraints that involve it.

If every variable is arc consistent, the graph is **arc consistent**.

The most popular algorithm to enforce arc consistency is the **AC-3** algorithm.

It uses a queue to store the arcs that need to be checked. On each iteration, it removes an arc from the queue and checks if the domain of the variables involved in the arc has changed. If it has, it adds the arcs that involve the variable to the queue.
If the domain of a variable is reduced to zero, the algorithm stops and returns _false_ because there is an inconsistency.

```python
def AC_3(csp)
"""
    return false if an inconsistency is found and true otherwise
"""
    queue = {(X_i, X_j) for each constraint C(X_i, X_j)}

    while queue is not empty
        (X_i, X_j) = queue.pop()

        if REVISE(csp, X_i, X_j)
            if X_i.domain is empty
                return False
            for each X_k in NEIGHBORS(X_i) - {X_j}
                queue.add((X_k, X_i))
    return True

def REVISE(csp, X_i, X_j)
"""
    return true if we revise the domain of X_i
"""
    revised = False

    for each x in X_i.domain
        if not exists y in X_j.domain such that (x, y) satisfies the constraint
            delete x from X_i.domain
            revised = True

    return revised

```

The complexity of the REVISE is $O(d^2)$, so the complexity of the AC-3 algorithm is $O(ed^3)$, where $e$ is the number of edges and $d$ is the maximum domain size.

### 4.1.3 Path Consistency

**Path consistency** is a stronger form of consistency that ensures that for every assignment between two variables, these is a consistent assignment for the rest of the variables.

Given a set of two variables $X_i$ and $X_j$, is path consistent with respect to a third variable $X_k$ if for every assignment of $X_i$ and $X_j$ there is a consistent assignment for $X_k$.

### 4.1.4 k-Consistency

A CSP is **k-consistent** if for any set of $k-1$ variables, there is a consistent assignment for the $k$-th variable.

## 4.2 Backtracking Search

If after the constraint propagation the CSP is not solved, we can use a **Backtracking Search** to find a solution.

The algorithm works by assigning a value to a variable and then propagating the constraints. If the assignment is consistent, the algorithm continues with the next variable. If the assignment is not consistent, the algorithm backtracks and tries a different value.

The algorithm can be implemented using a recursive function.

```python
def BACKTRACKING-SEARCH(csp)
"""
    return a solution or failure
"""
    return RECURSIVE-BACKTRACKING({}, csp)

def RECURSIVE-BACKTRACKING(assignment, csp)
"""
    return a solution or failure
"""
    if assignment is complete
        return assignment

    var = SELECT-UNASSIGNED-VARIABLE(assignment, csp)
    for each value in ORDER-DOMAIN-VALUES(var, assignment, csp)
        if value is consistent with assignment
            assignment[var] = value
            inferences = INFERENCE(csp, var, value)
            if inferences is not failure
                assignment.update(inferences)
                result = RECURSIVE-BACKTRACKING(assignment, csp)
                if result is not failure
                    return result
            assignment.remove(var)
            assignment.remove(inferences)
    return failure
```

This algorithm is called **Chronological Backtracking**.

### 4.2.1 Variable and Value Selection

To select the variable to assign, we can use a _fail-first_ strategy. This is useful to prune a branch tree because it reduces the number of possible values to try before an inconsistency is found.

Some strategies to select the variable are:

- **Minimum Remaining Values** (MRV): Select the variable with the fewest remaining values in its domain.
- **Degree Heuristic**: Select the variable that is involved in the largest number of constraints.

Once a variable is selected, we need to select a value to assign. This is done with a _fail-last_ strategy, which selects the value that is most likely to lead to a solution.

To select the value we can use:

- **Least Constraining Value**: Select the value that is less involved in constraints with other variables.

### 4.2.2 Inference

**Inference** is a technique that allows to reduce the domain of the variables based on the assignment of a variable.

One type of inference is **Forward Checking** that after assigning a value to a variable, removes the values that are inconsistent with the assignment from the domain of the neighbors.

This algorithm only checks the neighbors of the variable that has been assigned a value.

To solve this, we can use the **Maintain Arc Consistency** algorithm that calls the AC-3 algorithm, but instead of checking all the arcs, it only checks the arcs that involve the variable that has just been assigned a value. If the domain of a variable is reduced its neighbors are added to the queue, propagating the constraints.

### 4.2.3 Backjumping

When a failure is found, the chronological backtracking algorithm backtracks to the most recent variable that has been assigned a value and tries a different value.

A different approach is **Backjumping**, which backtracks to a variable that might fix the problem.
To do so we keep track of the variables that are involved in the failure (_conflict set_) and when backtracking we jump to the most recent variable that is involved in the failure.

A backtracking algorithm that uses backjumping is called **Conflict-Directed Backjumping**.

### 4.2.4 Constraint Learning

**Constraint Learning** is a technique that allows to learn from the failures and avoid them in the future.

When a failure is found, the algorithm can learn from the failure and add a new constraint that avoids the failure.

## 4.3 Local Search

**Local Search** is a technique that doesn't explore the entire search space, but instead, it starts from an initial configuration and tries to improve it by moving to a neighboring configuration.

The algorithm selects a conflicted variable and assigns a value that reduces the number of conflicts (**min-conflict** heuristic). This is done iteratively until a solution is found.

Some techniques to improve the search are:

- **Tabu Search**: Avoids moving to a configuration that has been visited recently by keeping a list of visited configurations.
- **Constraint Weighting**: Assigns a weight to the constraints and focus the search on the most important constraints. After each iteration, the weights are updated based on the number of conflicts.
