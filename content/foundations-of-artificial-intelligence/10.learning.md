---
title: "Chapter 9: Learning"
description: "Machine learning concepts, supervised learning, unsupervised learning, and reinforcement learning"
authors:
  - "Andrea Lunghi"
  - "Niccol√≤ Papini"
slug: "learning"
---

The improvements and techniques used to make the agent learn depends on:

- Which component to improve
- What prior knowledge is available
- What data and feedback is available

As said in [Chapter Two](./3.agent.md), the agent has different components and all of them can be learned. The agent components are:

- A map from condition to actions
- A way to retrieve properties from the percept sequence
- Information about the way the world evolves and the result of the Actions
- A utility function to indicate the desirability of the states
- Action-Value information to indicate the desirability of an action
- Goal to describe the most desirable state
- A problem generator, critic, and learning component

## 9.1 Type of Learning

Based on the type of the type of feedback that the agent receive with the data, we can have different types of learning:

### 9.1.1 Supervised Learning

In **Supervised Learning** the agent receives a set of examples of the input and the output (**Labels**) and the agent has to learn the function that maps the input to the output.

### 9.1.2 Unsupervised Learning

In **Unsupervised Learning** the agent receives a set of input and the agent has to learn the structure and patterns of the data.

One common task is the **Clustering** that is the task of grouping the data based on the similarity.

### 9.1.3 Reinforcement Learning

In **Reinforcement Learning** the agent receives a _feedback_ based on the actions that the agent performs.

The goal is to maximize the long-term reward.

Some things to take into are:

- The optimal action is context-dependent.
- Actions might have long-term consequences.
- Short-term consequences of optimal action might be negative.

The environment must be **Markov Decision Process** (MDP) so the next state and the reward must be dependent only on the current state and the action ($P(s_{t+1}, r_{t+1} | s_t, a_t)$).

#### Value Function

The agent need to compute a _action-value_ function that map state-action to the expected payoff.

$$Q(s_t, a_t) = E[r_t | s_t, a_t]$$

or with _state-value_ function that map state to the expected payoff.

$$V(s_t) = E[r_t | s_t]$$

The reward can be **discounted** by a factor $\gamma$ to give more importance to the immediate reward.

$$E[R_t] = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \leq r_{max} \frac{1}{1 - \gamma}$$

We can decompose the state-value function into immediate reward plus the discounted value of the next state (**Bellman Equation**).:

$$V(s) = E[r_{t+1} + \gamma V(s_{t+1}) | s_t = s]$$

And the action-value function can be similarly decomposed:

$$Q(s, a) = E[r_{t+1} + \gamma V(s_{t+1}) | s_t = s, a_t = a]$$

#### Policy

To select the action to take. This is done by the _policy_ $\pi(s_t)$.

The policy can be:

- **Deterministic**: The policy will select the action with the highest expected payoff.($\pi: S \to A$)
- **Stochastic**: The policy will map each action to a probability of being selected. ($\pi: S \times A \to [0, 1]$)

The agent need to find a trade-off between _exploration_ and _exploitation_ (exploration-exploitation dilemma).

- **Exploration**: The agent need to explore the environment to find the best action.
- **Exploitation**: The agent need to exploit the knowledge that has already.

The agent can use the **$\epsilon$-greedy** policy that select the best action with probability $1 - \epsilon$ and a random action with probability $\epsilon$.

#### Q-Learning

The **Q-Learning** is a reinforcement learning algorithm that learns the action-value function.

The function is represented by a table that at the beginning is initialized with random values.

The value of the table is updated based on the reward and the value of the next state.

$$Q(s_t, a_t) = Q(s_t, a_t) + \beta [r_{t+1} + \gamma \max_{a \in A} Q(s_{t+1}, a) - Q(s_t, a_t)]$$

Where $\beta$ is the learning rate and $\gamma$ is the discount factor.

In the real world it's impossible to have a table with all the possible states and actions. To solve this problem we can use a **Function Approximation** to approximate the action-value function.
